% == this file is about Deep Neural Network reference

% ==================================================================
%                     Summary
% ==================================================================
%{{{
@misc{DL_CS231_Andrej,
    title        = {{Stanford University CS231n: Convolutional Neural Networks for Visual Recognition}},
    author       = {Andrej Karpathy},
    howpublished = {\url{http://cs231n.github.io/neural-networks-3/}}
}
@book{DL_B2009_Haykins,
  title     = {{Neural Networks and Learning Machines}},
  author    = {Haykin, Simon S.},
  year      = {2009},
  publisher = {Pearson Upper Saddle River, NJ, USA:}
}
@article{DL_APSIPA2012_Li,
  title   = {Three classes of deep learning architectures and their applications: a tutorial survey},
  author  = {Deng, Li},
  journal = {APSIPA transactions on signal and information processing},
  year    = {2012},
}
@inproceedings{DL_IJCAI2005_Hinton,
  author    = {Geoffrey E.~Hinton},
  title     = {What kind of graphical model is the brain?},
  booktitle = ijcai,
  pages     = {1765--1775},
  year      = {2005},
  abstract  = {deep learning basis},
} 
@book{DL_MIT2016_Goodfellow,
  title     = {{Deep Learning}},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  publisher = {MIT Press},
  note      = {\url{http://www.deeplearningbook.org}},
  year      = {2016},
}
% ==== tools
@inproceedings{DL_ACMMM2014_Caffe,
  title     = {Caffe: Convolutional architecture for fast feature embedding},
  author    = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  booktitle = acmmm,
  pages     = {675--678},
  year      = {2014},
}
@inproceedings{DL_OSDI2016_TensorFlow,
  title     = {{TensorFlow}: A System for Large-scale Machine Learning},
  author    = {Abadi, Mart\'{\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and others},
  booktitle = osdi,
  year      = {2016},
  pages     = {265--283},
} 
% ==== benchmark
@article{DL_IJCV2015_Olga,
  title   = {{ImageNet} Large Scale Visual Recognition Challenge},
  author  = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C.~Berg and Li Fei-Fei},
  journal = ijcv,
  volume  = {115},
  number  = {3},
  pages   = {211-252},
  year    = {2015},
}
%}}}


% ==================================================================
%                     Training -- DNN
% ==================================================================
%{{{
@article{DL_Nature1986_Rumelhart,
  title   = {Learning representations by back-propagating errors},
  author  = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  journal = {Nature},
  volume  = {323},
  number  = {6088},
  pages   = {533--536},
  year    = {1986},
}
@inproceedings{DL_FLAIRS2002_Nasr,
  title     = {Cross Entropy Error Function in Neural Networks: Forecasting Gasoline Demand},
  author    = {Nasr, George E. and Badr, E.~A. and Joun, C.},
  booktitle = {FLAIRS Conference},
  pages     = {381--384},
  year      = {2002},
}
@article{DL_NECO2006_Hinton,
  title   = {A fast learning algorithm for deep belief nets},
  author  = {Geoffrey E.~Hinton and Simon Osindero and Yee Whye Teh},
  journal = neco,
  volume  = {18},
  number  = {7},
  pages   = {1527--1554},
  year    = {2006},
}
@inproceedings{DL_AISTATS2010_Glorot,
  title     = {Understanding the difficulty of training deep feedforward neural networks},
  author    = {Glorot, Xavier and Bengio, Yoshua},
  booktitle = aistats,
  volume    = {9},
  pages     = {249--256},
  year      = {2010},
}
@inproceedings{DL_ICML2010_Nair,
  title     = {Rectified linear units improve restricted boltzmann machines},
  author    = {Nair, Vinod and Hinton, Geoffrey E.},
  booktitle = icml,
  pages     = {807--814},
  year      = {2010},
}
@inproceedings{DL_NIPSW2010_Lamblin,
  title     = {Important Gains from Supervised Fine-tuning of Deep Architectures on Large Labeled Sets},
  author    = {Pascal Lamblin and Yoshua Bengio},
  booktitle = {NIPS Workshop on Deep Learning and Unsupervised Feature Learning},
  year      = {2010},
  abstract  = {finetune},
}
@inproceedings{DL_NIPS2012_Krizhevsky,
  title     = {{ImageNet} Classification with Deep Convolutional Neural Networks},
  author    = {Alex Krizhevsky and Ilya Sutskever and Geoffrey E.~Hinton},
  booktitle = nips,
  pages     = {1097--1105},
  year      = {2012},
  abstract  = {deep learning image classification},
}
@inproceedings{DL_ECCV2014_Zeiler,
  title     = {Visualizing and understanding convolutional networks},
  author    = {Zeiler, Matthew D. and Fergus, Rob},
  booktitle = eccv,
  pages     = {818--833},
  year      = {2014},
}
@article{DL_JMLR2014_Nitish,
  title   = {Dropout: a simple way to prevent neural networks from overfitting.},
  author  = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal = jmlr,
  volume  = {15},
  number  = {1},
  pages   = {1929--1958},
  year    = {2014},
}
@inproceedings{DL_AISTATS2015_Choromanska,
  title     = {The Loss Surfaces of Multilayer Networks},
  author    = {Choromanska, Anna and Henaff, MIkael and Mathieu, Michael and Ben Arous, Gerard and LeCun, Yann},
  booktitle = aistats,
  pages     = {192--204},
  year      = {2015},
}
@article{DL_ICLR2015_Karen,
  title   = {Very deep convolutional networks for large-scale image recognition},
  author  = {Simonyan, Karen and Zisserman, Andrew},
  journal = iclr,
  year    = {2015},
}
@article{DL_TSP2015_Giryes,
  title     = {Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?},
  author    = {Giryes, Raja and Sapiro, Guillermo and Bronstein, Alex M.},
  journal   = tsp,
  volume    = {64},
  number    = {13},
  pages     = {3444--3457},
  year      = {2015},
  publisher = {IEEE},
}
@article{DL_ARXIV2016_Mishkin,
  title   = {Systematic evaluation of {CNN} advances on the {ImageNet}},
  author  = {Mishkin, Dmytro and Sergievskiy, Nikolay and Matas, Jiri},
  journal = arxiv,
  year    = {2016},
}
# ==== CNNSGD
@article{DL_CMMP1964_Polyak,
  title     = {Some methods of speeding up the convergence of iteration methods},
  author    = {Polyak, Boris T.},
  journal   = {USSR Computational Mathematics and Mathematical Physics},
  volume    = {4},
  number    = {5},
  pages     = {1--17},
  year      = {1964},
  publisher = {Elsevier},
}
@incollection{DL_BC2012_Bottou,
  title     = {Stochastic gradient descent tricks},
  author    = {Bottou, L{\'e}on},
  booktitle = {{Neural networks: Tricks of the Trade}},
  editor    = {Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  pages     = {421--436},
  year      = {2012},
  publisher = {Springer},
}
@article{DL_NIPS1995_Moody,
  title   = {A simple weight decay can improve generalization},
  author  = {Moody, J. and Hanson, S. and Krogh, Anders and Hertz, John A.},
  journal = nips,
  volume  = {4},
  pages   = {950--957},
  year    = {1995},
}
@article{DL_ICML2013_Sutskever,
  title   = {On the importance of initialization and momentum in deep learning.},
  author  = {Sutskever, Ilya and Martens, James and Dahl, George E. and Hinton, Geoffrey E.},
  journal = icml,
  volume  = {28},
  pages   = {1139--1147},
  year    = {2013},
}
%}}}


% ==================================================================
%                    Structure -- DNN 
% ==================================================================
% ==== RNN
@incollection{DL_BC2001_Sepp,
  title     = {Gradient flow in recurrent nets: the difficulty of learning long-term dependencies},
  author    = {Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo and Schmidhuber, J{\"u}rgen},
  booktitle = {{A Field Guide to Dynamical Recurrent Neural Networks}},
  editor    = {Kolen, John F. and Kremer, Stefan C.},
  publisher = {IEEE Press},
  year      = {2001},
}
% ==== GAN
@incollection{DL_NIPS2014_Ian,
  title     = {Generative Adversarial Nets},
  author    = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle = nips,
  pages     = {2672--2680},
  year      = {2014},
}


% ==================================================================
%                      Inference Speedup -- DNN
% ==================================================================
% ==== convolution speedup
%{{{
@article{SPEED_ARXIV2014_Chetlur,
  title   = {{cuDNN}: Efficient primitives for deep learning},
  author  = {Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
  journal = arxiv,
  year    = {2014},
}
@inproceedings{SPEED_CVPR2015_Liu,
  title     = {Sparse Convolutional Neural Networks},
  author    = {Liu, Baoyuan and Wang, Min and Foroosh, Hassan and Tappen, Marshall and Penksy, Marianna},
  booktitle = cvpr,
  pages     = {806--814},
  year      = {2015},
}
@inproceedings{SPEED_CVPR2016_Lavin,
  title     = {Fast Algorithms for Convolutional Neural Networks},
  author    = {Lavin, Andrew and Gray, Scott},
  booktitle = cvpr,
  pages     = {4013--4021},
  year      = {2016},
  abstract  = {winograd-based convolution},
}
@article{SPEED_ARXIV2017_Shi,
  title   = {Speeding up Convolutional Neural Networks By Exploiting the Sparsity of Rectifier Units},
  author  = {Shi, Shaohuai and Chu, Xiaowen},
  journal = arxiv,
  year    = {2017},
}
@inproceedings{SPEED_ICML2017_Cho,
  title     = {{MEC}: memory-efficient convolution for deep neural network},
  author    = {Cho, Minsik and Brand, Daniel},
  booktitle = icml,
  year      = {2017},
}
@inproceedings{SPEED_ICLR2017_Park,
  title     = {Faster cnns with direct sparse convolutions and guided pruning},
  author    = {Park, Jongsoo and Li, Sheng and Wen, Wei and Tang, Ping Tak Peter and Li, Hai and Chen, Yiran and Dubey, Pradeep},
  booktitle = iclr,
  year      = {2017},
}
%}}}
% ==== Pruning / Hashing
%{{{
@inproceedings{SPEED_ICML2015_Chen,
  title     = {Compressing neural networks with the hashing trick},
  author    = {Chen, Wenlin and Wilson, James and Tyree, Stephen and Weinberger, Kilian and Chen, Yixin},
  booktitle = icml,
  pages     = {2285--2294},
  year      = {2015},
}
@inproceedings{SPEED_NIPS2015_Han,
  title     = {Learning both weights and connections for efficient neural network},
  author    = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle = nips,
  pages     = {1135--1143},
  year      = {2015},
}
@article{SPEED_ICLR2016_Han,
  title     = {Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author    = {Han, Song and Mao, Huizi and Dally, William J.},
  booktitle = iclr,
  year      = {2016},
}
@inproceedings{SPEED_ISCA2016_Han,
  title     = {{EIE}: efficient inference engine on compressed deep neural network},
  author    = {Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A. and Dally, William J.},
  booktitle = isca,
  pages     = {243--254},
  year      = {2016},
}
@inproceedings{SPEED_ISCA2017_Yu,
  title     = {Scalpel: Customizing dnn pruning to the underlying hardware parallelism},
  author    = {Yu, Jiecao and Lukefahr, Andrew and Palframan, David and Dasika, Ganesh and Das, Reetuparna and Mahlke, Scott},
  booktitle = isca,
  pages     = {548--560},
  year      = {2017},
}
%}}}
% ==== low rank approximation (LRA)
%{{{
@article{SPEED_ICLR2014_Davis,
  title     = {Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks},
  author    = {Davis, Andrew S. and Arel, Itamar},
  booktitle = iclr,
  year      = {2014},
}
@inproceedings{SPEED_ICML2014_Si,
  title     = {Memory efficient kernel approximation},
  author    = {Si, Si and Hsieh, Cho-Jui and Dhillon, Inderjit},
  booktitle = icml,
  pages     = {701--709},
  year      = {2014},
}
%}}}



% ==================================================================
%                    Applications -- DNN
% ==================================================================
% ==== Applications in CV
%{{{
@incollection{DL_BC2012_Yoshua,
  title     = {Practical recommendations for gradient-based training of deep architectures},
  author    = {Bengio, Yoshua},
  booktitle = {{Neural Networks: Tricks of the Trade}},
  editor    = {Orr, Genevieve B. and M{\"u}ller, Klaus-Robert},
  pages     = {437--478},
  year      = {2012},
  publisher = {Springer},
}
@inproceedings{DL_CVPR2012_Huang,
  title     = {Learning hierarchical representations for face verification with convolutional deep belief networks},
  author    = {Huang, Gary B. and Lee, Honglak and Learned-Miller, Erik},
  booktitle = cvpr,
  pages     = {2518--2525},
  year      = {2012},
}
@inproceedings{DL_ICCV2013_Yi,
  title     = {Hybrid deep learning for face verification},
  author    = {Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
  booktitle = iccv,
  pages     = {1489--1496},
  year      = {2013},
}
@inproceedings{DL_CVPR2016_Xiao,
  title     = {Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification},
  author    = {Tong Xiao and Hongsheng Li and Wanli Ouyang and Xiaogang Wang},
  booktitle = cvpr,
  pages     = {1249--1258},
  year      = {2016},
  abstract  = {deep learning basis},
} 
%}}}


